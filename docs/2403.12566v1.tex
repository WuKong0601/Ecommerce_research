\documentclass[conference]{acmart}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}

\begin{document}

Context-based Fast Recommendation Strategy for Long User
Behavior Sequence in Meituan Waimai
Zhichao Fengâˆ—
Beijing University of Posts and
Telecommunications
Beijing, China
fengzc@bupt.edu.cnJunJie Xieâˆ—
Meituan
Beijing, China
xiejunjie02@meituan.comKaiyuan Li
Beijing University of Posts and
Telecommunications
Beijing, China
tsotfsk@bupt.edu.cn
Yu Qin
Meituan
Beijing, China
qinyu12@meituan.comPengfei Wangâ€ 
Beijing University of Posts and
Telecommunications
Beijing, China
wangpengfei@bupt.edu.cnQianzhong Li
Bin Yin
Meituan
Beijing, China
{liqianzhong,yinbin05}@meituan.com
Xiang Liâ€ 
Meituan
Beijing, China
lixiang245@meituan.comWei Lin
Meituan
Beijing, China
linwei31@meituan.comShangguang Wang
Beijing University of Posts and
Telecommunications
Beijing, China
sgwang@bupt.edu.cn
ABSTRACT
In the recommender system of Meituan Waimai, we are dealing with
ever-lengthening user behavior sequences, which pose an increas-
ing challenge to modeling user preference effectively. A number
of existing sequential recommendation models struggle to capture
long-term dependencies, or they exhibit high complexity, both of
which make it difficult to satisfy the unique business requirements
of Meituan Waimaiâ€™s recommender system.
To better model user interests, we consider selecting relevant
sub-sequences from usersâ€™ extensive historical behaviors based
on their preferences. In this specific scenario, weâ€™ve noticed that
the contexts in which users interact have a significant impact on
their preferences. For this purpose, we introduce a novel method
called Context-based FastRecommendation Strategy (referred
to as CoFARS ) to tackle the issue of long sequences. We first
identify contexts that share similar user preferences with the tar-
get context and then locate the corresponding Points of Interest
(PoIs) based on these identified contexts. This approach eliminates
the necessity to select a sub-sequence for every candidate PoI,
thereby avoiding high time complexity. Specifically, we implement
âˆ—Both authors contributed equally to this research.
â€ Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
WWW â€™24 Companion, May 13â€“17, 2024, Singapore, Singapore
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0172-6/24/05. . . $15.00
https://doi.org/10.1145/3589335.3648334a prototype-based approach to pinpoint contexts that mirror simi-
lar user preferences. To amplify accuracy and interpretability, we
employ Jensenâ€“Shannon(JS) divergence of PoI attributes such as
categories and prices as a measure of similarity between contexts.
Subsequently, we construct a temporal graph that encompasses
both prototype and context nodes to integrate temporal informa-
tion. We then identify appropriate prototypes considering both
target contexts and short-term user preferences. Following this,
we utilize contexts aligned with these prototypes to generate a
sub-sequence, aimed at predicting CTR and CTCVR scores with
target attention.
Since its inception in 2023, this strategy has been adopted in
Meituan Waimaiâ€™s display recommender system, leading to a 4.6\%
surge in CTR and a 4.2\% boost in GMV.
CCS CONCEPTS
â€¢Information systems â†’Recommender systems .
KEYWORDS
Click-Through Rate Prediction, User Preference Modeling, Long
Sequential User Behavior Data
ACM Reference Format:
Zhichao Feng, JunJie Xie, Kaiyuan Li, Yu Qin, Pengfei Wang, Qianzhong Li,
Bin Yin, Xiang Li, Wei Lin, and Shangguang Wang. 2024. Context-based Fast
Recommendation Strategy for Long User Behavior Sequence in Meituan
Waimai. In Companion Proceedings of the ACM Web Conference 2024 (WWW
â€™24 Companion), May 13â€“17, 2024, Singapore, Singapore. ACM, New York, NY,
USA, 9 pages. https://doi.org/10.1145/3589335.3648334
1 INTRODUCTION
In the intricate ecosystem of Meituan Waimai, Chinaâ€™s premier local
retail and instant delivery platform, the deployment of an advanced,arXiv:2403.12566v1  [cs.IR]  19 Mar 2024

WWW â€™24 Companion, May 13â€“17, 2024, Singapore, Singapore Zhichao Feng et.al.
Figure 1: A Userâ€™s behaviors vary in two characteristics. (a)
Geographical location: user interests in different locations
(such as companies, and homes) are quite different. (b) Din-
ing time: preference of users at breakfast time (e.g. bun) is
different from that at lunchtime (e.g. fast food) and midnight
time (e.g. BBQ).
location-based recommender system is crucial for connecting mil-
lions of users with an array of services, from food delivery to phar-
maceuticals. The task is made more challenging by the fact that 27\%
of users have engaged with the app over 1,000 times in the last year,
necessitating a highly efficient process for managing extensive user
behavior sequences. Traditional models such as RNNs [ 7,13] falter
in maintaining long-term memory across these sequences, while
attention-based methods [ 12,28] are hindered by the significant
computational demands of processing Meituan Waimaiâ€™s long user
behavior sequences.
Recently, two-stage recommendation models [ 3,4,21] have
emerged. Based on the target item, the first stage filters out a sub-
sequence, and the second stage performs fine-grained modeling
on this sub-sequence. However, these models primarily rely on
static features such as item attributes for sequence filtering, with-
out adequately considering the dynamic factors that influence user
preferences, such as the context during user interaction. Further-
more, the target attention-based approach integral to these models
necessitates traversing the entire sequence for each candidate item,
incurring substantial computational overhead. Complicating mat-
ters further, these methods rely on the semantic similarity between
item embeddings to infer user preferencesâ€”a process that can be
compromised by the quality of the embeddings, thus affecting the
reliability and efficiency of recommendations. In fact, the recom-
mendation process in Meituan Waimai is uniquely influenced by
contextual factors like location, time, and weather, diverging from
the attribute-centric focus common in e-commerce [ 15,27]. This
context-driven user preference landscape underscores the inad-
equacy of traditional methods. Analysis of user interaction data
shown in Fig. 1 reveals that preferences vary considerably across
different contexts: for instance, the preference for lighter meals
like salads or rice noodles is markedly higher at company, while
heartier options such as BBQ and hot pot are favored at home. Meal-
time further influences choices, with buns and soymilk popular for
breakfast, and fast food or pasta preferred for lunch, transitioning
to fried skewers and BBQ for dinner.
To overcome these challenges, in this work, we adhere to the
two-stage paradigm, concentrating on the first stage which involves
selecting a sub-sequence from the userâ€™s behavior sequence. Con-
sidering the context-dependency characteristic of Meituan Waimai,we propose to select PoIs that are interacted with under the target
context from the userâ€™s historical behaviors, which is a method inde-
pendent of candidate PoIs. However, if we only retain behaviors that
completely align with the target context, the retrieved sub-sequence
will be excessively short, thereby limiting the effectiveness of the
recommendation system. For this purpose, we propose a Context-
based FastRecommendation Strategy (denoted as CoFARS ) to ad-
dress long sequence issues in the Meituan Waimai recommender
system. Our goal is to identify contexts similar to the target con-
text based on user preferences. We employ Jensenâ€“Shannon (JS)
divergence to measure the similarity of PoI attribute distributions
across different contexts, enhancing interpretability and accuracy.
To reduce complexity, we use a prototype-based approach, where
prototypes act as centroids of semantically similar neighbors in the
preference representation space. Since prior knowledge of proto-
types is not available, inspired by transfer learning [ 40], we con-
vert latent representations into probability distributions using an
encoder and align them with the actual JS divergence between
different contexts obtained from log data using MSE loss. We also
build a temporal graph of prototype and context nodes to introduce
temporal information. By taking into account the userâ€™s short-term
preference, we select the prototypes that align with the target con-
text and retrieve the relevant sub-sequence from the user behavior
sequence.
To verify the effectiveness of the proposed model, we conduct
both offline evaluation experiments on a large user-logged dataset
and online A/B testing experiments on the Meituan Waimai app.
Experimental results show that our proposed framework outper-
forms the comparison baselines in both metrics. Our contributions
can be summarized as follows:
â€¢We propose a candidate-agnostic method to model long se-
quences based on contexts. To the best of our knowledge,
this is the first initiative to employ contexts in the modeling
of long sequences in this domain.
â€¢We propose a prototype-based approach to address the chal-
lenges of long sequences. Leveraging our proposed probabil-
ity encoder and graph-based temporal aggregator, we can
effectively pinpoint contexts that display preferences akin
to the target context.
â€¢We conduct both offline evaluations on large dataset and
online A/B tests on the online platform of Meituan Waimai
to demonstrate the effectiveness of our approach.
2 RELATED WORK
2.1 Context-aware Recommendation
Context-aware Recommendation aims to leverage rich context
information to improve the recommendation performance [ 32].
Firstly, Rendle [ 26] introduce the factorization machine(FM) to
mine the interaction between context features to enhance the rec-
ommendation. Inspired by FM, some works [ 9,11,19] have been
proposed to model the second-order interactions between context
features while others [ 17,33] utilize deep neural networks to model
high-order interactions between features. Although these meth-
ods leverage feature interactions to achieve great performance,
their interaction components are empirically predefined. To solve
this problem, Cheng et al. [ 5] propose to use logarithmic neural

Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai WWW â€™24 Companion, May 13â€“17, 2024, Singapore, Singapore
networks (LNN) [ 8] to adaptively model the arbitrary-order in-
teractions, while Tian et al. propose EulerNet [ 29] to model the
feature interactions in a complex vector space through Euler for-
mula, where the feature interactions in the model are adaptively
learned from data.
Recently, several studies also propose to improve the perfor-
mance of sequential recommendation by integrating contextual
information [ 16,18,23,25,36]. However, despite their effectiveness,
these methods predominantly rely on the self-attention mechanism,
which falls short in addressing the challenges posed by modeling
long-term sequences.
2.2 Long User Behavior Sequence Modeling
As the sequence lengthens, the modelâ€™s performance correspond-
ingly enhances. [ 21]. Existing methods modeling long user behavior
sequences can be mainly divided into two categories: RNN-based
methods and two-stage methods. In RNN-based methods, some
models utilize memory networks to memorize usersâ€™ historical be-
havior sequences [ 20,24], such as HPMN [ 24] and MIMN [ 20].
Recently, Wu et al. [ 35] propose a linear transformer mechanism
based on kernel functions to model long sequences.
Recently, two-stage methods have attracted increasing attention.
Pi et al. [ 21] first propose a two-stage method for long sequence
problems, which utilizes a General Search Unit (GSU) module to
retrieve items related to the target item from long sequences in
hard or soft manners, and then use Exact Search Unit (ESU) mod-
ule to finely model the retrieved sub-sequence. To speed up the
GSU stage, ETA [ 4] proposes to use locality-sensitive hashing to
transform item embeddings into binary vectors, with Hamming
distance measuring item similarity. Similarly, with the hashing
method, SDIM consolidates and normalizes item embeddings with
the same hash as the target, deducing user preference for the target
item. However, the relevance between the items retrieved by GSU
and ESU may be relatively low. Therefore, Chang et al.[ 3] proposed
consistency-preserving GSU (CP-GSU) to enhance the consistency
of the two stages. Different from the above retrieval-based GSU
method, UBCS [ 37] proposes a clustering-based method to extract
representative sub-sequences. In contrast to their studies,our ap-
proach involves utilizing context rather than the target item for
retrieval during the GSU stage.
3 METHODOLOGY
In this section, we first introduce the problem formulation of rec-
ommendations in Meituan Waimai. We then describe the proposed
CoFARS model in detail.
3.1 Problem Formulation
LetUdenote a set of users and Edenote a set of PoIs, where |U|
and|E|are the numbers of users and PoIs. For each user ğ‘¢âˆˆU,
we useğ‘’ğ‘¢
1:ğ‘›=
ğ‘’ğ‘¢
1,ğ‘’ğ‘¢
2,Â·Â·Â·,ğ‘’ğ‘¢
ğ‘–,Â·Â·Â·,ğ‘’ğ‘¢ğ‘›	
to represent the interaction
sequence of PoIs, where ğ‘’ğ‘¢
ğ‘–âˆˆE.
Each user interaction in the sequence is accompanied by various
contextual features, such as dining time, location, weather, etc. We
refer to the combination of these contextual features as a context,
likeâŸ¨breakfast, office, sunny, Â·Â·Â·âŸ©. We denote byCğ‘¢the set of
distinct contexts in which the user has interacted, where Cğ‘¢={Cğ‘¢
1,Cğ‘¢
2,Â·Â·Â·,Cğ‘¢
|Cğ‘¢|}. Additionally, we use A={a1,a2,...,a|A|}
to represent the set of PoI attributes (e.g., categories, prices, etc.),
where|A|is the number of attribute fields, and each element within
it represents a set of possible values that attribute can take. We
discretize the continuous attributes into several buckets. To denoise
PoIs unrelated to user preferences based on target contexts, while
also avoiding issues of inaccurate modeling due to overly short
remaining sequences, our goal is to identify contexts with similar
user preferences to the target one, which will then be used for
filtering in the user behavior sequence.
For simplicity, we describe the technical details of CoFARS for a
single userğ‘¢, and it is straightforward to extend the formulas to a
set of users. Hence, we drop subscript ğ‘¢in the notations for concise
presentation.
3.2 Probability Encoder
Within the Meituan Waimai recommender system, myriad context
features such as geographic location, dining time, weather, and
holiday presence significantly influence user preferences. Tradi-
tional approaches, including cosine similarity of context embed-
dings, aimed at identifying contexts with analogous user inclina-
tions and selecting sub-sequences pertinent to the target context,
encounter challenges including a lack of interpretability and a re-
liance on the quality of embeddings. Indeed, the task of accurately
capturing user preferences in a specific context using solely latent
representations without supervision poses a significant challenge.
Nonetheless, we observe that user preferences within particular
contexts are more accurately represented through probability dis-
tributions across PoI attributes, such as category and price. This
insight leads us to employ probability distribution consistency as
a measure for comparing preferences across different contexts,
thereby enhancing both interpretability and accuracy. A prevalent
method involves calculating the Kullback-Leibler (KL) divergence.
However, to circumvent its inherent asymmetry, we opted for the
symmetric alternative, the Jensen-Shannon (JS) divergence. We
useD(Cğ‘–)=ğ‘(ğ‘1,ğ‘2,Â·Â·Â·,ğ‘|A||Cğ‘–)to denote the distribution of
attribute values{ğ‘1,ğ‘2,Â·Â·Â·,ğ‘|A|}under contextCğ‘–, which is a|A|-
dimension vector recording the proportions of corresponding dis-
crete attribute values that could be obtained from the log data.The
equation of divergence between given context Cğ‘–and contextCğ‘—
comes as follows:
KL(Cğ‘–,Cğ‘—)=âˆ‘ï¸
ğ‘1âˆˆa1âˆ‘ï¸
ğ‘2âˆˆa2Â·Â·Â·âˆ‘ï¸
ğ‘|A|âˆˆa|A|D(Cğ‘–)logD(Cğ‘–)
D(Cğ‘—)(1)
JS(Cğ‘–,Cğ‘—)=1
2 KL(Cğ‘–,Cğ‘—)+KL(Cğ‘—,Cğ‘–)(2)
For clarity, the matching process is shown in Fig. 2 by visualizing
the distribution of order probability over the price and category
dimensions. The order probability is calculated by log data, and it
can be viewed as userâ€™s interest in PoI in the context.
The described method involves calculating the similarity in pref-
erences between the target context and historical contexts, followed
by the selection of a subset of the top similar contexts. However,
this strategy is not without its limitations, particularly in its inabil-
ity to address the challenge of context cold-start. Moreover, the

WWW â€™24 Companion, May 13â€“17, 2024, Singapore, Singapore Zhichao Feng et.al.
Context ğ¶!Context ğ¶"Match ?
Context ğ¶#Match !
JS-divergence matching
Figure 2: An example of the JS divergence calculation based
on the PoI attributes between different contexts.
effectiveness of the model is substantially affected by the quantity
of contexts selected for each target context, a figure that fluctuates
unpredictably across various target contexts. In fact, it is impor-
tant to recognize that preferences derived from prolonged behavior
within a particular context tend to remain relatively stable. There-
fore, we introduce the concept of preference prototypes, drawing
inspiration from prototype learning methodologies referenced in
prototype learning literature [ 14]. We define the collection of these
prototypes asO={ğ‘œ1,ğ‘œ2,Â·Â·Â·,ğ‘œ|O|}, where|O|represents the to-
tal number of prototypes. These prototypes serve as the centroids
within the preference representation space, grouping semantically
akin neighbors. Contexts whose preferences closely match the same
prototype are considered analogous. However, as our understand-
ing of the prototypes is limited to their latent representations and
we lack explicit prior knowledge about them, directly deriving their
probability distributions across PoI attributes is not feasible. To
calculate the JS divergence between prototypes and context prefer-
ences, inspired by transfer learning, we design a probability encoder
that maps latent representations into probability distributions over
PoI attributes. We align the distributions with the ground truth JS
divergence between contexts calculated based on log data. Specifi-
cally, we use Ë†cğ‘–âˆˆRğ‘‘to represent the latent representation of the
global preference under context Cğ‘–, whereğ‘‘represents the number
of representation dimensions, and it is shared by all users to reduce
the number of parameters. The userâ€™s personalized preference is de-
noted as cğ‘–=u+Ë†cğ‘–. To approximate the probability distribution, we
employ P(cğ‘–)=MLP(cğ‘–), utilizing a Multilayer Perceptron (MLP)
model that incorporates a sigmoid activation function. The MLPâ€™s
output dimensionality is meticulously set to match the aggregate
of value counts across all attributes. Based on the estimated proba-
bility distributions, we define the estimated JS divergence between
representations cğ‘–andcğ‘—as follows:fKL(cğ‘–,cğ‘—)=ğ‘‘âˆ‘ï¸
ğ‘˜=1ğ‘ğ‘–,ğ‘˜Â·logğ‘ğ‘–,ğ‘˜
ğ‘ğ‘—,ğ‘˜(3)
eJS cğ‘–,cğ‘—=1
2(fKL(P(cğ‘–),P(cğ‘—))+fKL(P(cğ‘—),P(cğ‘–))) (4)
whereğ‘ğ‘–,ğ‘˜represent the ğ‘˜-th element of cğ‘–. We aim to encourage
the estimated JS divergence to approximate the ground truth value
define in Eq. 1 obtained from log data. To achieve this, we add the
following constraint:
Lğ‘€ğ‘†ğ¸=1
|C|2|C|âˆ‘ï¸
ğ‘–=1|C|âˆ‘ï¸
ğ‘—=1(JS Cğ‘–,Cğ‘—âˆ’eJS(cğ‘–,cğ‘—))2(5)
By minimizingLğ‘€ğ‘†ğ¸, we can align the estimated JS divergence
and the ground truth. In consequence, we could directly calculate
the similarity between latent representations:
ğ‘ ğ‘–ğ‘š(cğ‘–,cğ‘—)=1âˆ’eJS(cğ‘–,cğ‘—) (6)
Itâ€™s noteworthy that contexts are dynamically clustered, and
surplus prototypes contain virtually no contexts. Consequently,
when the number of prototypes surpasses a specific threshold, the
modelâ€™s performance remains relatively stable. In other words, the
modelâ€™s efficacy is not solely contingent on this hyperparameter, a
fact we have substantiated in the experiments detailed below.
Besides, we can calculate the JS divergence among prototypes
using the probability encoder. To avoid excessive concentration of
the prototypes, we introduce an independence loss to constrain
prototype representations. Similarly, we use Ë†oğ‘–âˆˆRğ‘‘to represent
the representations shared by users, which follow the same distri-
bution as the representations of context preference, and oğ‘–=Ë†oğ‘–+u
to represent personalized representation for prototype ğ‘œğ‘–.To pro-
mote the distinctiveness of different prototypes, we encourage their
separation through JS divergence:
Lğ¼ğ‘ğ·=âˆ’1
|O|2|O|âˆ‘ï¸
ğ‘–=1|O|âˆ‘ï¸
ğ‘—=1eJS oğ‘–,oğ‘—(7)
By minimizingLğ¼ğ‘ğ·, we can make the prototypes more evenly
distributed in the latent space.
3.3 Graph-based Temporal Aggregator
While the probability encoder effectively clusters contexts, it over-
looks the temporal dynamics embedded within sequences. This
oversight becomes particularly critical in handling long sequences
where traditional RNN-based models may grapple with memory
loss over time, and the computational demand of attention-based
models becomes prohibitively intricate. Recently, graph neural net-
works have been applied to sequential recommendation, construct-
ing connected graphs of items to handle long user behavior se-
quence [ 2], modeling long-term item dependencies. A common
method for graph construction utilizes the co-occurrence relation-
ships between items to establish connections. However, due to the
sparsity of items in user behavior sequence, the graphs often de-
generate into linear structures, limiting the effectiveness of GNNs.
Consequently, GNN applications are mainly found in areas with

Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai WWW â€™24 Companion, May 13â€“17, 2024, Singapore, Singapore
frequent item repetition, like session-based recommendations [ 34].
Nevertheless, within context sequences, the restricted diversity of
contexts and the extensive sequence length lead to repeated con-
texts, forming an optimal scenario for GNN modeling. Accordingly,
a temporal graph is constructed based on the sequential order of
contexts, wherein nodes denote contexts, and edges signify the co-
occurrences among adjacent contexts. This framework proficiently
encapsulates the inherent sequential dynamics within user behav-
ior, offering a nuanced approach to understanding and modeling
user interactions.
â€¦â€¦Embedding LayerGRU Layerâ€¦â€¦Build GraphContextItemAggregatedPrototypesğ‘!lRetrievalSub-sequenceâ€¦
Reparametrize JS Divergence
Figure 3: The overall architecture of Context-based User fast
Recommendation Strategy( CoFARS for short) for long se-
quences in recommender system of Meituan Waimai.
To incorporate temporal dimensions into prototypes, we embed
prototype nodes within the graph. However, fully connecting pro-
totype and context nodes risks creating an unwieldy graph size.
To mitigate this, we suggest a filtering method that retains only
edges with similarity scores above a certain threshold. However, the
conventional hard-coding filtering mechanism is not differentiable,
obstructing effective backpropagation training. To bypass this, we
adopt the Gumbel Softmax trick from prior work [ 10], facilitating
differentiable learning over discrete outputs. The equation for each
prototypeğ‘–and context ğ‘—is as follows:
ğ›½=ğ‘ ğ‘–ğ‘š(oğ‘–,cğ‘—) (8)
P(ğ›½)=exp((log(ğ›½+ğ‘”ğ‘)/ğœğ‘)
Ã1
ğ‘=0exp(log(ğ›½ğ‘(1âˆ’ğ›½)1âˆ’ğ‘)+ğ‘”ğ‘)/ğœğ‘)(9)whereğ‘”ğ‘represents a noise sampled from a Gumbel distribution,
and the temperature parameter ğœğ‘controls its sharpness. Addi-
tionally, the similarity between each prototype-context pair is pre-
calculated and saved in the storage, eliminating the need for real-
time computation online. For the pair where P(ğ›½)=1, we add
a bidirectional edge between the nodes. Next, we utilize Graph
Attention Network [ 31], which could deal with directed graphs
and unseen nodes, to aggregate the graph and embed temporal
information into the node representation:
H0=[o1;Â·Â·Â·;o|O|;c1;Â·Â·Â·;c|C|] (10)
ğ›¼ğ‘™
ğ‘– ğ‘—=exp(LeakyReLU(ğ‘ ğ‘–ğ‘š(Hğ‘™
ğ‘–Wğ‘™,Hğ‘™
ğ‘—Wğ‘™))
Ã
ğ‘˜âˆˆNğ‘–exp(LeakyReLU(ğ‘ ğ‘–ğ‘š(Hğ‘™
ğ‘–Wğ‘™,Hğ‘™
ğ‘˜Wğ‘™)))(11)
Hğ‘™+1
ğ‘–=ğœ(âˆ‘ï¸
ğ‘—âˆˆNğ‘–ğ›¼ğ‘– ğ‘—Hğ‘™
ğ‘—W). (12)
where Hğ‘™
ğ‘–âˆˆRğ‘‘andWğ‘™âˆˆRğ‘‘Ã—ğ‘‘represent the ğ‘–-th node rep-
resentation and the parameters in the ğ‘™-th layer, respectively. ğœ
represents the ReLU activation function. Nğ‘–denotes the predeces-
sor nodes of node ğ‘–.Afterğ¿times of aggregation, we can obtain the
ğ‘–-th aggregated prototype representation eoğ‘–=Hğ¿
ğ‘–andğ‘—-th context
preference representation ecğ‘—=Hğ¿
|O|+ğ‘—integrated with temporal
information.
3.4 Learning and Discussion
Once having the aggregated node representations, we identify the
prototypes that fit with the target context by focusing on the userâ€™s
most recent ğ‘Ÿactivities e(ğ‘›âˆ’ğ‘Ÿ):ğ‘›={e(ğ‘›âˆ’ğ‘Ÿ),e(ğ‘›âˆ’ğ‘Ÿ+1),Â·Â·Â·,eğ‘›},
whereğ‘Ÿâ‰ªğ‘›. We encode the userâ€™s short-term behavior using
GRU model [7]:
l=GRU(e(ğ‘›âˆ’ğ‘Ÿ),e(ğ‘›âˆ’ğ‘Ÿ+1),Â·Â·Â·,eğ‘›) (13)
where lrepresents the userâ€™s short-term preference. Next, we
assume that the target context of the user is Cğ‘¡.Then, we calculate
the relation between prototypes and the target context:
ğ‘§ğ‘¡
ğ‘–=P(ğ‘ ğ‘–ğ‘š(eoğ‘–,ecğ‘¡)(eoğ‘–Â·l)) (14)
where the dot represents inner product operation. As there is
no concept of probability distribution on attributes for PoIs, we
compute the similarity between prototypes and candidate PoI using
the inner product. To ensure effective training, we utilize binary
cross entropy loss as Eq. 15 to provide supervision for the entire
process.
ğ‘ ğ‘¡
ğ‘–,ğ‘£=ğ‘§ğ‘¡
ğ‘–Â·(eoğ‘–Â·v) (15)
Lğ‘…ğ¸ğ¶=1
|ğ‘‰|âˆ‘ï¸
vâˆˆğ‘‰|eO|âˆ‘ï¸
ğ‘–=1[âˆ’ğ‘¦Â·logğ‘ ğ‘¡
ğ‘–,ğ‘£âˆ’(1âˆ’ğ‘¦)Â·log(1âˆ’ğ‘ ğ‘¡
ğ‘–,ğ‘£)](16)
whereğ‘‰denotes the training set, and ğ‘¦=1for positive samples
whileğ‘¦=0otherwise. The final loss is as follows:
L=Lğ‘…ğ¸ğ¶+ğ›¾Â·Lğ‘€ğ‘†ğ¸+ğœ†Â·Lğ¼ğ‘ğ· (17)

WWW â€™24 Companion, May 13â€“17, 2024, Singapore, Singapore Zhichao Feng et.al.
whereğ›¾andğœ†control the weights of the MSE loss and the
independence loss, respectively. The overall learning algorithm for
the proposed method is given in Alg. 1.
Algorithm 1 Learning algorithm for CoFARS
Input: Training epoch ğ‘‡ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
1:Initialize model parameters Î˜â†random values;
2:forğ‘‡ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› epochs do
3: Construct a temporal graph of contexts based on the userâ€™s historical
behavior;
4: Calculate the similarity between prototypes and contexts according
to Eq. 6;
5: Integrate prototype nodes into the graph, establishing connections
between prototypes and contexts according to Eq. 8;
6: Perform message passing between nodes based on Eq. 10;
7: Derive the userâ€™s short-term preferences through Eq.13;
8: Identify the prototypes that match the target context using Eq. 14;
9: Optimize the model parameters by minimizing the loss function
Eq. 17;
10:end for
11:return All optimal parameters in Î˜.
In the next phase, for each prototype ğ‘œğ‘–in the setOwhereğ‘§ğ‘¡
ğ‘–=1,
we collect the contexts that are encompassed by it. Within these
contexts, we identify and select a sub-sequence of PoIs with which
the user has engaged. This selected sub-sequence is then processed
using target attention [ 39], to intricately model and capture user
interest. The model is trained under the cross-entropy loss function.
For the online prediction of a userâ€™s subsequent visit within the
target contextCğ‘¡, we implement diverse strategies. For previously
encountered context, we use the pre-calculated similarity between
prototype-context pairs. For cold-start context, we employ the rep-
resentation cğ‘¡to calculate its similarity with the prototypes. Based
on this representation, we emulate the learning process mentioned
in this section to select sub-sequence and model user interest.
The proposed CoFARS method selects a sub-sequence based on
contexts with similar preferences to the target context independent
of candidate items. In this section, we analyze the time complexity
of inference time in the GSU stage. In the offline phase, it calculates
the similarity between prototypes and context preferences with a
time complexity of O(| C|Â·|O |), where|C|is much smaller than the
sequence length ğ‘›, and|O|is a constant. The time complexity of
graph aggregation is ğ‘‚(ğ‘›Â·ğ¿). In the online phase, we need ğ‘‚(ğ‘Ÿ+
|O|) to model the short-term preference to obtain the selected sub-
sequence and use it to retrieve relevant prototypes. It is evident that
our approach has a significantly better time complexity compared
to other two-stage models, which have a time complexity of ğ‘‚(ğµÂ·ğ‘›),
whereğµrepresents the number of candidates.
4 EXPERIMENT
4.1 Experiemental Setup
Dataset. For offline experiments, we collect training data by log
data of Meituan Waimai from April 1st to April 30th, 2023, and
the validation data is constructed by the samples on May 9th, 2023.
Especially, sample data contains user profile/statistical features, sta-
tistical features of PoIs, raw user-behavior features, labels (whetherclick or not), and so on. The raw user-behavior features include all
PoIs that users have interacted with over the past three years. In
addition, we use context features including geographical location,
mealtime, weather, and the presence of holidays. For PoI attributes,
we consider category, price, quality, and delivery time. The statistics
of our dataset are shown in Tab. 1, where Recent Avg. refers to the
average click records number of users in the training set, while
Historical Avg. refers to the average interaction records number
of users over the past three years, which are used to model user
interests.
Table 1: Statistics of the dataset
Field Size
#Users 0.24 billion
#PoIs 4.37 million
#Records 36 billion
#Recent Avg. 150
#Historical Avg. 4,423
Baseline. To evaluate the effectiveness of our approach, we com-
pare our model against two types of baselines, including three
traditional models, and five enhanced models for long sequences.
Specifically, we consider three traditional models:
â€¢Avg-Pooling [6] is a basic deep learning model for CTR
prediction. It performs average pooling to integrate behavior
embeddings.
â€¢DIN[39] is an attention-based method that only utilizes
short-term user behavior sequences to model user interests.
â€¢DIEN [38] integrates GRU with a candidate-based attention
mechanism to model user interests.
For enhanced models for long sequences, we consider the following
five baselines:
â€¢MIMN[ 20]: utilizes a memory network to capture long-term
user interest.
â€¢SIMâ„ğ‘ğ‘Ÿğ‘‘[21] gets relevant interacted PoIs by category id.
â€¢SIMğ‘ ğ‘œğ‘“ğ‘¡[21] searches top-k interacted PoIs by the similarity
of PoI embeddings. For our experiments, the well-trained
PoI embeddings are obtained from our Recall System.
â€¢SDIM [1] aggregates and normalizes the embeddings of
items with the same hash as the target item in the user
behavior sequence to derive the user preference related to
the target item.
â€¢TWIN [3] is an improved version of SIM, which enhances
the consistency of embeddings in GSU and ESU stages.
Parameter Setting. For a fair comparison, we adopt the following
setting for all methods: the number of training epochs is set to
500 and the batch size is set to 128. All embedding parameters are
randomly initialized in the range of (0, 1), and the learning rate is
tuned in the range of {1e-4, 5e-4, 1e-3} and the embedding size is
tuned in the range of {8, 16, 32}. For other parameters, we follow
the settings in the original paper or source code.
ForCoFARS , we implement it based on PyTorch with Adam
optimizer. For Eq. 8, the ğœğ‘is set to tuned in {1e-3, 1e-2, 0.1, 1, 2}.
Theğ›¾andğœ†in Eq. 17 are set to 5e-2 and 1e-3, respectively. And the

Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai WWW â€™24 Companion, May 13â€“17, 2024, Singapore, Singapore
number of prototypes is tuned in {1, 10, 20, 30, 40, 60, 80}, which is
studied in the subsequent section.
4.2 Performance Comparison
In this section, we compare the performance of our model with
the baselines. The overall performance of our proposed CoFARS
and the baselines are reported in Tab. 2. We have the following
observations:
For traditional models, we observe that Avg-Pooling has the
poorest performance. By employing target attention to assign vary-
ing weights to PoIs in the behavior sequence, we discover that DIN
and DIEN outperform models without attention, demonstrating the
significance of diminishing the impact of irrelevant PoIs. Further-
more, by incorporating temporal information, DIEN outperforms
DIN, which validates the importance of integrating sequential in-
formation in CoFARS .
In terms of enhanced recommenders for long sequences, their
performance surpasses that of DIN and DIEN, which are short-term
models. This indicates the critical role of long sequences in more
precisely capturing user interests. We also observed that MIMN
has the least effective performance among all enhanced models.
This could be due to its approach of updating memory by assign-
ing weights to PoIs in the sequence via a soft attention manner,
where potential noise accumulation can significantly hinder the
sequence learning process. Consequently, its performance lags be-
hind two-stage methods that employ a hard-coding approach. A
similar observation could be explored in [22, 30]. Additionally, de-
spite a reduction in time complexity, SDIM outperforms SIM â„ğ‘ğ‘Ÿğ‘‘
and exhibits comparable performance to SIM ğ‘ ğ‘œğ‘“ğ‘¡, validating the
efficacy of the hashing method proposed by SDIM. Furthermore,
TWIN excels over SDIM as it not only enhances the consistency
of the two-stage representation but also employs target attention
instead of normalizing the PoI embeddings for sequential modeling,
thereby amplifying the correlation between user interest and the
target PoI.
Finally, CoFARS outperforms all baselines and outperforms
TWIN by 0.77\% on CTR AUC and 0.63\% on CTCVR AUC, respec-
tively, which proves the effectiveness of the proposed CoFARS . By
introducing contextual information, CoFARS , with the proposed
probability encoder and graph-based temporal aggregator, CoFARS
can effectively and efficiently identify contexts with similar pref-
erences, according to which it selects PoIs relevant to the target
context.
4.3 Ablation Study
Our proposed CoFARS method measures the similarity between the
prototypes and the context preferences based on JS divergence and
constrains the latent representations through several losses while
introducing temporal information into the representation through
GNN.To verify the effectiveness of such a design, we consider the
following variants of our model for comparison through offline
experiments:
â€¢CoFARSğ¼ğ‘ƒ: This variant represents using the inner product
instead of JS divergence as the similarity measure.
â€¢CoFARSÂ¬ğ‘€ğ‘†ğ¸: This variant represents removing Lğ‘€ğ‘†ğ¸ for
comparison with CoFARS andCoFARSğ¼ğ‘ƒ.Table 2: Performance comparison between our CoFARS and
baselines. The best performance of each field is highlighted
in boldface. The improvement of CoFARS against the best
baseline is consistently significant at 0.05 level.
Model CTR AUC CTCVR AUC
Traditional ModelsAvg-Pooling 0.6993 0.7204
DIN 0.7052 0.7263
DIEN 0.7069 0.7281
Enhanced Models
for Long SequencesMIMN 0.7083 0.7287
SIMâ„ğ‘ğ‘Ÿğ‘‘ 0.7097 0.7300
SIMğ‘ ğ‘œğ‘“ğ‘¡ 0.7116 0.7321
SDIM 0.7123 0.7325
TWIN 0.7133 0.7336
CoFARS 0.7188 0.7382
â€¢CoFARSÂ¬ğ¼ğ‘ğ·: This variant represents removing the inde-
pendence constraint of prototypes.
â€¢CoFARSÂ¬ğºğ‘‡ğ´: This variant represents removing the graph-
based temporal aggregator and directly using the original
representations for subsequent operations instead of the
aggregated ones.
Table 3: Performance comparison of CoFARS and different
variants
Model CTR AUC CTCVR AUC
CoFARSğ¼ğ‘ƒ 0.7146 0.7341
CoFARSÂ¬ğ‘€ğ‘†ğ¸ 0.7128 0.7326
CoFARSÂ¬ğ¼ğ‘ğ· 0.7149 0.7343
CoFARSÂ¬ğºğ‘‡ğ´ 0.7141 0.7339
CoFARS 0.7188 0.7382
The performance comparison among CoFARS and its variants is
shown in Tab. 3. From the table, we have the following observations.
Firstly, regarding the similarity measure, we found that using the
inner product instead of the similarity defined in Eq. 6 leads to a
decrease in performance. This proves that calculating JS divergence
based on the attribute distributions of the context indeed brings a
more accurate similarity measure. In addition, if we only remove
the alignment constraint in Eq. 5, this will lead to poor performance,
which indicates that it is difficult for the model to find contexts
with similar preferences by calculating JS divergence based on
random and unaligned representations. Secondly, we found that
removingLğ¼ğ‘ğ· leads to a degradation in performance as well.
The underlying reason might be that without the independence
constraint, the distribution of each prototype becomes concentrated,
making it challenging for the model to distinguish between diverse
user preferences. Thirdly, we observe a performance decline when
the graph-based temporal aggregator is removed. This confirms the
importance of integrating transit relationships between contexts
into the representations, as its absence leads to a loss of temporal
information.

WWW â€™24 Companion, May 13â€“17, 2024, Singapore, Singapore Zhichao Feng et.al.
/uni00000014/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000014/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000014/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000015/uni00000013/uni00000026/uni00000037/uni00000035/uni00000003/uni00000024/uni00000038/uni00000026
/uni00000014/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000016/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000016/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000017/uni00000013/uni00000026/uni00000037/uni00000026/uni00000039/uni00000035/uni00000003/uni00000024/uni00000038/uni00000026
Figure 4: Performance variation of different prototype num-
bers
4.4 Analysis on Effect of Prototype Number
Recall that in our proposed CoFARS, the number of prototypes is
a hyperparameter, in this senction, we evaluate the effectiveness
of prototypes in CoFARS by altering the number of prototypes | O|.
The outcomes are depicted in Fig.4. When |O|=1, we only retain
the PoIs that have been interacted with in the target context pre-
viously. Remarkably, CoFARS consistently surpasses the baseline
for various |O| values, except when |O|=1. Optimal performance
is attained when |O| is approximately 40, suggesting that contexts
with similar user preferences are appropriately assigned to pro-
totypes. On the contrary, setting | O| to 1 results in a substantial
performance drop. This can be attributed to the filtered sequences
are too short and monotonous to capture usersâ€™ interest diversity
and its evolution.
4.5 Visualization Analysis
In this section, to delve deeper into whether the learned represen-
tations of our model can genuinely discern contexts with similar
user preferences, we have undertaken a visualization of the simi-
larity among contexts. Specifically, we have selected a user as an
example, whose mealtimes encompass breakfast, lunch, and dinner,
and whose habitual locations include his home (a residential area)
and his company (a business district). By utilizing the Cartesian
product, we are able to generate six distinct contexts. Leveraging
the well-trained embedding, we employ the Eq. 2 to compute the
similarity of user preferences across these contexts and perform
normalization. The visualization result of all representations is
shown in Fig. 5.
In the figure, weâ€™ve uncovered several insights. Foremost, it is
clear that within the same contextual features, users demonstrate
similar preferences. For example, when mealtime is constant while
the location changes, the context similarity is higher than in com-
pletely unrelated contexts. Specifically, ğ‘ ğ‘–ğ‘š(âŸ¨ğ‘,â„âŸ©,âŸ¨ğ‘,ğ‘œâŸ©)exceeds
ğ‘ ğ‘–ğ‘š(âŸ¨ğ‘,â„âŸ©,âŸ¨ğ‘™,ğ‘œâŸ©)andğ‘ ğ‘–ğ‘š(âŸ¨ğ‘,â„âŸ©,âŸ¨ğ‘‘,ğ‘œâŸ©). This pattern is consistent
across all mealtimes. With a static location, similar trends are ob-
served, which is likely attributable to consistent preferences for the
same mealtime or location. For instance, users may prefer heav-
ier lunches and lighter meals for breakfast and dinner, meanwhile
choose pricier PoIs at home and cheaper options at the company.
Secondly, breakfast orders at home and the company are more
similar than that of lunch and dinner, possibly due to the limited
/uni0000000b/uni00000045/uni0000000f/uni0000004b/uni0000000c /uni0000000b/uni00000045/uni0000000f/uni00000046/uni0000000c /uni0000000b/uni0000004f/uni0000000f/uni0000004b/uni0000000c /uni0000000b/uni0000004f/uni0000000f/uni00000046/uni0000000c /uni0000000b/uni00000047/uni0000000f/uni0000004b/uni0000000c /uni0000000b/uni00000047/uni0000000f/uni00000046/uni0000000c/uni0000000b/uni00000045/uni0000000f/uni0000004b/uni0000000c
/uni0000000b/uni00000045/uni0000000f/uni00000046/uni0000000c
/uni0000000b/uni0000004f/uni0000000f/uni0000004b/uni0000000c
/uni0000000b/uni0000004f/uni0000000f/uni00000046/uni0000000c
/uni0000000b/uni00000047/uni0000000f/uni0000004b/uni0000000c
/uni0000000b/uni00000047/uni0000000f/uni00000046/uni0000000c
/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013Figure 5: Similarity between preferences under different con-
texts. The x-axis and y-axis both denote the contexts, ğ‘stands
for breakfast, ğ‘™stands for lunch, ğ‘‘stands for dinner, â„repre-
sents home, and ğ‘represents company.
breakfast variety compared to lunch and dinner. Lastly, the min-
imum similarity across all contexts exceeds 0.3, indicating basic
preferences like favoring lower-priced PoIs.
4.6 Online A/B Testing
CoFARS is conducted in the recommender system of Meituan
Waimai for online A/B testing from 2023-05 to 2023-06. During
almost a month of testing, compared with the baseline, the last
version of our online-serving model, CoFARS obtained 4.6\% CTR
and 4.2\% GMV promotion. The significant improvement demon-
strates the effectiveness of our proposed approach. CoFARS has
been deployed online and serving the main traffic in our real system
now.
5 CONCLUSION
In this paper, we tackle the challenge of long sequences in the
Meituan Waimai recommender system. The main contribution of
our work, CoFARS , lies in its integration of context information and
the introduction of a fast candidate-agnostic two-stage approach to
address this problem. Specifically, CoFARS introduces a similarity
measure based on JS divergence, which enhances interpretability
and accuracy. Moreover, it leverages GNN to incorporate temporal
information into node representation. Additionally, CoFARS com-
bines short-term interests to retrieve prototypes that match the
target context and utilizes contexts with similar user preferences
to obtain a relevant sub-sequence. Our experiments, conducted in
both offline and online settings, demonstrate the effectiveness of
our proposed model. To the best of our knowledge, we are the first
to leverage context information for modeling long sequences in
this domain.
ACKNOWLEDGMENTS
This research work was supported by General Program of the Na-
tional Natural Science Foundation of China (No.62372059). We

Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai WWW â€™24 Companion, May 13â€“17, 2024, Singapore, Singapore
would like to thank the anonymous reviewers for their valuable
comments.
REFERENCES
[1]Yue Cao, Xiaojiang Zhou, Jiaqi Feng, Peihao Huang, Yao Xiao, Dayao Chen, and
Sheng Chen. 2022. Sampling Is All You Need on Modeling Long-Term User
Behaviors for CTR Prediction. In CIKM . ACM, 2974â€“2983.
[2]Jianxin Chang, Chen Gao, Yu Zheng, Yiqun Hui, Yanan Niu, Yang Song, Depeng
Jin, and Yong Li. 2021. Sequential Recommendation with Graph Neural Networks.
InSIGIR . ACM, 378â€“387.
[3]Jianxin Chang, Chenbin Zhang, Zhiyi Fu, Xiaoxue Zang, Lin Guan, Jing Lu, Yiqun
Hui, Dewei Leng, Yanan Niu, Yang Song, and Kun Gai. 2023. TWIN: TWo-stage
Interest Network for Lifelong User Behavior Modeling in CTR Prediction at
Kuaishou. In KDD . ACM, 3785â€“3794.
[4]Qiwei Chen, Changhua Pei, Shanshan Lv, Chao Li, Junfeng Ge, and Wenwu
Ou. 2021. End-to-End User Behavior Retrieval in Click-Through RatePrediction
Model. abs/2108.04468 (2021).
[5]Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Adaptive Factorization
Network: Learning Adaptive-Order Feature Interactions. In AAAI . AAAI Press,
3609â€“3616.
[6]Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks for
YouTube Recommendations. In RecSys . ACM, 191â€“198.
[7]BalÃ¡zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.
2016. Session-based Recommendations with Recurrent Neural Networks. In
ICLR .
[8]J. Wesley Hines. 1996. A logarithmic neural network architecture for unbounded
non-linear function approximation. In ICNN . IEEE, 1245â€“1250.
[9]Fuxing Hong, Dongbo Huang, and Ge Chen. 2019. Interaction-Aware Factoriza-
tion Machines for Recommender Systems. In AAAI . AAAI Press, 3804â€“3811.
[10] Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical Reparameterization
with Gumbel-Softmax. In ICLR (Poster) . OpenReview.net.
[11] Yu-Chin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Field-
aware Factorization Machines for CTR Prediction. In RecSys . ACM, 43â€“50.
[12] Wang-Cheng Kang and Julian J. McAuley. 2018. Self-Attentive Sequential Rec-
ommendation. In ICDM . 197â€“206.
[13] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017.
Neural Attentive Session-based Recommendation. In CIKM . ACM, 1419â€“1428.
[14] Junnan Li, Pan Zhou, Caiming Xiong, and Steven C. H. Hoi. 2021. Prototypical
Contrastive Learning of Unsupervised Representations. In ICLR . OpenReview.net.
[15] Xiaochen Li, Jian Liang, Xialong Liu, and Yu Zhang. 2022. Adversarial Filter-
ing Modeling on Long-term User Behavior Sequences for Click-Through Rate
Prediction. In SIGIR . ACM, 1969â€“1973.
[16] Yang Li, Tong Chen, Peng-Fei Zhang, and Hongzhi Yin. 2021. Lightweight Self-
Attentive Sequential Recommendation. In CIKM . ACM, 967â€“977.
[17] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and
Guangzhong Sun. 2018. xDeepFM: Combining Explicit and Implicit Feature
Interactions for Recommender Systems. In KDD . ACM, 1754â€“1763.
[18] Chang Liu, Xiaoguang Li, Guohao Cai, Zhenhua Dong, Hong Zhu, and Lifeng
Shang. 2021. Non-invasive Self-attention for Side Information Fusion in Sequen-
tial Recommendation. CoRR abs/2103.03578 (2021).
[19] Wantong Lu, Yantao Yu, Yongzhe Chang, Zhen Wang, Chenhui Li, and Bo Yuan.
2020. A Dual Input-aware Factorization Machine for CTR Prediction. In IJCAI .
ijcai.org, 3139â€“3145.
[20] Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Practice on
Long Sequential User Behavior Modeling for Click-Through Rate Prediction. In
KDD . ACM, 2671â€“2679.
[21] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang
Zhu, and Kun Gai. 2020. Search-based User Interest Modeling with Lifelong
Sequential Behavior Data for Click-Through Rate Prediction. In CIKM . ACM,
2685â€“2692.
[22] Yuqi Qin, Pengfei Wang, and Chenliang Li. 2021. The World is Binary: Contrastive
Learning for Denoising Next Basket Recommendation. In SIGIR . 859â€“868.
[23] Ahmed Rashed, Shereen Elsayed, and Lars Schmidt-Thieme. 2022. Context and
Attribute-Aware Sequential Recommendation via Cross-Attention. In RecSys .
ACM, 71â€“80.
[24] Kan Ren, Jiarui Qin, Yuchen Fang, Weinan Zhang, Lei Zheng, Weijie Bian, Guorui
Zhou, Jian Xu, Yong Yu, Xiaoqiang Zhu, and Kun Gai. 2019. Lifelong Sequential
Modeling with Personalized Memorization for User Response Prediction. In SIGIR .
ACM, 565â€“574.
[25] Ruiyang Ren, Zhaoyang Liu, Yaliang Li, Wayne Xin Zhao, Hui Wang, Bolin
Ding, and Ji-Rong Wen. 2020. Sequential Recommendation with Self-Attentive
Multi-Adversarial Network. In SIGIR . ACM, 89â€“98.
[26] Steffen Rendle. 2010. Factorization Machines. In ICDM . IEEE Computer Society,
995â€“1000.
[27] Uriel Singer, Haggai Roitman, Yotam Eshel, Alexander Nus, Ido Guy, Or Levi,
Idan Hasson, and Eliyahu Kiperwasser. 2022. Sequential Modeling with MultipleAttributes for Watchlist Recommendation in E-Commerce. In WSDM . ACM,
937â€“946.
[28] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.
2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Repre-
sentations from Transformer. In CIKM . 1441â€“1450.
[29] Zhen Tian, Ting Bai, Wayne Xin Zhao, Ji-Rong Wen, and Zhao Cao. 2023. Euler-
Net: Adaptive Feature Interaction Learning via Eulerâ€™s Formula for CTR Predic-
tion. In SIGIR . ACM, 1376â€“1385.
[30] Xiaohai Tong, Pengfei Wang, Chenliang Li, Long Xia, and Shaozhang Niu. 2021.
Pattern-enhanced Contrastive Policy Learning Network for Sequential Recom-
mendation. In IJCAI . ijcai.org, 1593â€“1599.
[31] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
LiÃ², and Yoshua Bengio. 2018. Graph Attention Networks. In ICLR . OpenRe-
view.net.
[32] Norha M. Villegas, Cristian SÃ¡nchez, Javier DÃ­az-Cely, and Gabriel Tamura. 2018.
Characterizing context-aware recommender systems: A systematic literature
review. Knowl. Based Syst. 140 (2018), 173â€“200.
[33] Ruoxi Wang, Rakesh Shivanna, Derek Zhiyuan Cheng, Sagar Jain, Dong Lin,
Lichan Hong, and Ed H. Chi. 2021. DCN V2: Improved Deep \& Cross Network
and Practical Lessons for Web-scale Learning to Rank Systems. In WWW . ACM /
IW3C2, 1785â€“1797.
[34] Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. 2019.
Session-Based Recommendation with Graph Neural Networks. In AAAI . AAAI
Press, 346â€“353.
[35] Yongji Wu, Lu Yin, Defu Lian, Mingyang Yin, Neil Zhenqiang Gong, Jingren
Zhou, and Hongxia Yang. 2021. Rethinking Lifelong Sequential Recommendation
with Incremental Multi-Interest Attention. CoRR abs/2105.14060 (2021).
[36] Tingting Zhang, Pengpeng Zhao, Yanchi Liu, Victor S. Sheng, Jiajie Xu, De-
qing Wang, Guanfeng Liu, and Xiaofang Zhou. 2019. Feature-level Deeper
Self-Attention Network for Sequential Recommendation. In IJCAI . ijcai.org, 4320â€“
4326.
[37] Yuren Zhang, Enhong Chen, Binbin Jin, Hao Wang, Min Hou, Wei Huang, and
Runlong Yu. 2022. Clustering based Behavior Sampling with Long Sequential
Data for CTR Prediction. In SIGIR . ACM, 2195â€“2200.
[38] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu,
and Kun Gai. 2019. Deep Interest Evolution Network for Click-Through Rate
Prediction. In AAAI . AAAI Press, 5941â€“5948.
[39] Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma,
Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for
Click-Through Rate Prediction. In KDD . ACM, 1059â€“1068.
[40] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu
Zhu, Hui Xiong, and Qing He. 2021. A Comprehensive Survey on Transfer
Learning. Proc. IEEE 109, 1 (2021), 43â€“76.

\end{document}
